I decided to store the data in three ways: as a 2d array of the three pieces of info, as a map of movies to their ratings, and as a map of users to the movies they rated and the rating they gave. When determining popularity I decided to make popularity be an average of the ratings that a movie recieved. For similarity, I decided the number should be the difference in movie ratings for movies they shared subtracted from 5. With more time I would adjust popularity to take into account how many times it was rated and I would change similarity to detract from the score for movies that were not shared. 

The algorithm to determine how a user would rate a movie would need to find as many users as posible that had both seen similar movies and rated them in similar ways, and had seen the movie in question. We could then find a weighted average their ratings of the movie in question acording to how similar their tastes were to the original user. 

My algorithm should scale. The loadData method runs through the data once for O(n), popularity has O(n) where n will be the average number of ratings a movie recieved. popularity_list runs in O(n*m) time where n is the number of data points and m is the average number of ratings a movie recieved. similarity runs in O(n) time where n is the average number of movies a user rated. Similarity list runs in O(n*m) where n is the number of data points and m in the average number of movies a user rated. Because of this, execution time depends on how many pieces of data there are but more importantly, how many times a movie was rated and how many users rated movies because if less movies were rated more often and less users rated more movies, both the popularity_list and similarity_list would suffer in execution time because the respective m's would be higher. 